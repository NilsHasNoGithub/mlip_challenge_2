{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from library.models._legacy_timm_model import TimmModule\n",
    "from library.inference_utils import correct_img_rotation\n",
    "from library.config import TrainMetadata\n",
    "from library.data.utils import list_index\n",
    "\n",
    "ckpt_path = \"../rotation_model/checkpoint.ckpt\"\n",
    "\n",
    "train_metadata = TrainMetadata.from_yaml(\"../train_metadata.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = (\n",
    "    TimmModule.load_from_checkpoint(ckpt_path, pretrained=False, for_inference=True)\n",
    "    .cpu()\n",
    "    .eval()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4472\n",
      "../data/hotel-id-to-combat-human-trafficking-2022-fgvc9/train_images/83679/000022124.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "all_imgs = [os.path.join(\"..\", p) for p in list_index(train_metadata.images, train_metadata.val_idxs)]\n",
    "print(len(all_imgs))\n",
    "print(all_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.data.utils import read_img_rot, read_img\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from typing import Any\n",
    "from torchvision.transforms import Compose\n",
    "import PIL.Image as pil_img\n",
    "import random\n",
    "\n",
    "n_imgs = 200\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ImgRot:\n",
    "    img: Any\n",
    "    img_90: Any\n",
    "    img_180: Any\n",
    "    img_270: Any\n",
    "\n",
    "    def vals(self):\n",
    "        return [self.img, self.img_90, self.img_180, self.img_270]\n",
    "\n",
    "    def apply_transform(self, t: Compose) -> \"ImgRot\":\n",
    "        return ImgRot(*(t(pil_img.fromarray(i)) for i in self.vals()))\n",
    "\n",
    "\n",
    "\n",
    "def get_rot_img(img_p) -> ImgRot:\n",
    "    img = read_img(img_p)\n",
    "\n",
    "    rot_imgs = (read_img_rot(img_p, i) for i in range(1, 4))\n",
    "\n",
    "    return ImgRot(img, *rot_imgs)\n",
    "\n",
    "\n",
    "imgs = random.sample(all_imgs, k=n_imgs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4800e85a4a45425293fd2461e750a932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "t_transform = model.get_transform()\n",
    "\n",
    "SAVE_IMGS: Optional[str] = \".cache/rotcors\"\n",
    "PLOT_IMGS = False \n",
    "\n",
    "def bench_fn(typ, fn, args, kwargs, disable=False):\n",
    "    now = time.time()\n",
    "    result = fn(*args, **kwargs)\n",
    "    if not disable:\n",
    "        print(f\"{typ}, took: {time.time() - now}\")\n",
    "    return result\n",
    "\n",
    "for i_img, img_p in enumerate(tqdm(imgs)):\n",
    "    fig, plts = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "    img_rots = get_rot_img(img_p)\n",
    "\n",
    "    np_imgs = img_rots.vals()\n",
    "    t_imgs = img_rots.apply_transform(t_transform).vals()\n",
    "    for i_rot in range(4):\n",
    "        np_img = np_imgs[i_rot]\n",
    "        model_inp = t_imgs[i_rot]\n",
    "\n",
    "        sp = plts[0, i_rot]\n",
    "\n",
    "        model_inp = model_inp.reshape(1, *model_inp.shape)\n",
    "\n",
    "        pred = bench_fn(\"inference\", lambda: torch.argmax(model.forward(model_inp)[0, ...].detach().cpu()).item(), [], {}, disable=True)\n",
    "\n",
    "        # double work, but test if this function works\n",
    "        np_img_cor = bench_fn(\"corr_img\", lambda: correct_img_rotation(model, np_img), [], {}, disable=True)\n",
    "        sp.set_title(f\"Prediction angle: {90 * pred}, True angle {90 * i_rot}\")\n",
    "        sp.imshow(np_img)\n",
    "\n",
    "        sp.axes.get_xaxis().set_visible(False)\n",
    "        sp.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "        sp = plts[1, i_rot]\n",
    "        sp.set_title(\"Above corrected image\")\n",
    "        sp.imshow(np_img_cor)\n",
    "\n",
    "        sp.axes.get_xaxis().set_visible(False)\n",
    "        sp.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if PLOT_IMGS:\n",
    "            fig.show()\n",
    "\n",
    "        if SAVE_IMGS is not None:\n",
    "            os.makedirs(SAVE_IMGS, exist_ok=True)\n",
    "            fig.savefig(os.path.join(SAVE_IMGS, f\"{i_img}.png\"))\n",
    "\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4d60a3fcd887db86e846d76f7c6a18e00f62eb51f11e90b42aa96d6417b59e4"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 ('mlip_challenge_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
