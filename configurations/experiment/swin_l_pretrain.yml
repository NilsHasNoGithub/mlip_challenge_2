base:
  model_type: swin_large_patch4_window7_224_in22k
  experiment_name: pretraining
  num_epochs: 300
  optimizer: AdamW
  learning_rate: 1.0e-03
  weight_decay: 1.0e-06
  batch_size: 16
  # augmentation_preset: default
  augmentation_preset: fgvc8_winner
  gradient_accumulation: 16
  extra_model_params:
    drop_rate: 0.25
